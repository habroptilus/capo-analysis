{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# chord2vecを入力にcapo予測"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import sys,os\n",
    "sys.path.append(os.pardir)\n",
    "from tools.preprocess.common import CommonPreprocessor\n",
    "from tools.preprocess.interaction_matrix_generator import InteractionMatrixGenerator\n",
    "from tools.preprocess.bow_vectorizer import BOWVectorizer\n",
    "from scipy.sparse import csr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "orignal_path=Path(\"../data/preprocessed_50k.txt\")\n",
    "bow_threshold=10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "cp=CommonPreprocessor(rare_capo_list=['capo6', 'capo7', \"whole_down\"],test_rate=0.2,split_seed=0)\n",
    "songs=cp.get_song_list(orignal_path,shuffle=True)\n",
    "songs=cp.remove_rare_capo_song(songs)\n",
    "songs_train, songs_test=cp.split_dataset(songs,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "chord_stat_train=cp.retrieve_chord_stat(songs_train)\n",
    "capo_stat_train=cp.retrieve_capo_stat(songs_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'capo0': 15636,\n",
       "         'capo1': 5255,\n",
       "         'capo2': 6488,\n",
       "         'capo3': 5284,\n",
       "         'capo4': 3808,\n",
       "         'capo5': 2295,\n",
       "         'half_down': 1139})"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "capo_stat_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.base import TransformerMixin\n",
    "\n",
    "class LabelGenarater(TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.encoder= LabelEncoder()\n",
    "    \n",
    "    def fit(self,songs):\n",
    "        self.encoder.fit([song[\"rec_capo\"] for song in songs])\n",
    "        return self\n",
    "    \n",
    "    def transform(self,songs):\n",
    "        return self.encoder.transform([song[\"rec_capo\"] for song in songs])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "lg=LabelGenarater()\n",
    "y_train=lg.fit_transform(songs_train)\n",
    "y_test=lg.fit_transform(songs_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=BOWVectorizer(chord_stat=chord_stat_train,threshold=bow_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((39905, 62), (9977, 62))"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_bow=vectorizer.get_chord_features(songs_train)\n",
    "X_test_bow=vectorizer.get_chord_features(songs_test)\n",
    "X_train_bow.shape,X_test_bow.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7207577428084595\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/hikaru/.pyenv/versions/anaconda3-4.4.0/lib/python3.6/site-packages/sklearn/preprocessing/label.py:151: DeprecationWarning: The truth value of an empty array is ambiguous. Returning False, but in future this will result in an error. Use `array.size > 0` to check that an array is not empty.\n",
      "  if diff:\n"
     ]
    }
   ],
   "source": [
    "import lightgbm as lgb\n",
    "import numpy as np\n",
    "\n",
    "lgbm_cls = lgb.LGBMClassifier(objective='multiclass',num_class= 7)\n",
    "lgbm_cls.fit(X_train_bow, y_train)\n",
    "y_pred_bow  = lgbm_cls.predict(X_test_bow)\n",
    "# 精度 (Accuracy) を計算する\n",
    "accuracy = sum(y_test == y_pred) / len(y_test)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "embedding_dim=100\n",
    "min_count=5\n",
    "window_size=5\n",
    "iter_num=100\n",
    "\n",
    "# モデルを保存\n",
    "model_name=f\"../result/w2v/{embedding_dim}_{min_count}_{window_size}_{iter_num}.model\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import word2vec\n",
    "\n",
    "class Chord2VecAvg:\n",
    "    def __init__(self,model_name,vectorizer):\n",
    "        self.c2v = word2vec.Word2Vec.load(model_name)\n",
    "        self.bow_vectorizer=vectorizer \n",
    "        self.embeddings= self.create_embedding_mat()\n",
    "    \n",
    "    def transform(self,songs):\n",
    "        bow=self.bow_vectorizer.get_chord_features(songs_train)\n",
    "        return bow @ self.embeddings / np.expand_dims(np.sum(X_train_bow,axis=1),axis=1)\n",
    "        \n",
    "    def create_embedding_mat(self):\n",
    "        embedding_dim= len(self.c2v.wv.vectors[0])\n",
    "        embeddings=np.zeros((len(self.bow_vectorizer.chord_encoder),embedding_dim))\n",
    "        for chord, index in self.bow_vectorizer.chord_encoder.items():\n",
    "            embeddings[index] = self.c2v.wv[chord]\n",
    "        return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "c2v_vectorizer=Chord2VecAvg(model_name,vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "operands could not be broadcast together with shapes (39905,100) (39905,) ",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-88-35c58f5c0527>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mc2v_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msongs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-86-0bfb7b4a8c56>\u001b[0m in \u001b[0;36mtransform\u001b[0;34m(self, songs)\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msongs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0mbow\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbow_vectorizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_chord_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msongs_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mbow\u001b[0m \u001b[0;34m@\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0membeddings\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train_bow\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mcreate_embedding_mat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: operands could not be broadcast together with shapes (39905,100) (39905,) "
     ]
    }
   ],
   "source": [
    "c2v_vectorizer.transform(songs_train).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "39905"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
